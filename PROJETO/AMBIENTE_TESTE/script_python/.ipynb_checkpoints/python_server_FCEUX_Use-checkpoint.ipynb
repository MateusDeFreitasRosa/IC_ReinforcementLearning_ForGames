{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_server_FCEUX import Server\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting connection from emulator...\n",
      "Connected:  <socket.socket fd=2244, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 12345), raddr=('127.0.0.1', 56672)>\n"
     ]
    }
   ],
   "source": [
    "server = Server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonsMode(string):\n",
    "    a = string.decode()\n",
    "    return json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get pixeis image.\n",
    "params: {(int) down_sample -> 1, (int) len_max_x -> 255, (int) len_max_y -> 239, (int) len_min_x -> 1,\n",
    "(int) len_min_x -> 1, (bool) grayscale -> False}\n",
    "\n",
    "'''\n",
    "image = server.sendCommandAndReceiveOperation(json.dumps({'operation': 'getScreenShot',\n",
    "          'params': {\n",
    "              'grayscale': True,\n",
    "              'down_sample': 4,\n",
    "              'len_min_y': 45,\n",
    "              'len_max_y': 230\n",
    "          }\n",
    "      }))\n",
    "print('LenImage: {}'.format(len(image)))\n",
    "print(image)\n",
    "#printImage(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = image.decode()\n",
    "b = a.split('json')[1]\n",
    "c = json.loads(b)\n",
    "d = c['matriz']\n",
    "e = np.asarray(d, dtype=np.int16)\n",
    "plt.imshow(e, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict({'a':2}).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 2 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 3 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 4 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 5 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 6 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 7 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 8 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 9 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 10 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 11 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 12 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 13 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 14 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 15 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 16 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 17 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 18 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 19 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 20 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 21 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 22 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 23 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 24 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 25 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 26 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 27 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 28 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 29 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 30 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 31 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 32 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 33 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 34 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 35 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 36 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 37 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 38 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 39 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 40 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 41 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 42 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 43 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 44 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 45 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 46 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 47 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 48 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 49 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 50 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 51 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 52 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 53 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 54 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 55 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 56 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 57 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 58 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 59 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 60 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 61 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 62 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 63 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 64 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 65 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 66 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 67 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 68 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 69 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 70 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 71 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 72 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 73 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 74 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 75 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 76 - ['matriz', 'retorno']\n",
      "{}\n",
      "I: 77 - ['matriz', 'retorno']\n",
      "{}\n",
      "Requisitando novamente <class 'TimeoutError'>\n",
      "Except: 'NoneType' object is not subscriptable\n",
      "6.340348243713379\n",
      "I: 77\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Press Joypad\n",
    "'''\n",
    "import time\n",
    "import random\n",
    "begin = time.time()\n",
    "images = []\n",
    "i = 0\n",
    "while True:\n",
    "    try:\n",
    "        data = server.step('right', grayscale=True, downsample=2, min_y=45, max_y=230)\n",
    "        #print('Data')\n",
    "        #print(data)\n",
    "        #js = jsonsMode(data)\n",
    "        d = data['matriz']\n",
    "        e = np.asarray(d, dtype=np.int16)\n",
    "        if 'endgame' in data and data['endgame'] != 8:\n",
    "            print('END')\n",
    "            server.reset('Super_Mario_Bros.fc1', grayscale=True, downsample=2, min_y=45, max_y=230)\n",
    "        i+=1\n",
    "        print(\"I: {} - {}\".format(i, list(data.keys())))\n",
    "        print(data['retorno'])\n",
    "    except Exception as e:\n",
    "        print('Except: {}'.format(e))\n",
    "        break\n",
    "    \n",
    "end = time.time()\n",
    "print(end - begin)\n",
    "print('I: {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(number):\n",
    "    return ['down', 'left', 'right', 'A', 'B'][number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = jsonsMode(data)\n",
    "d = js['matriz']\n",
    "e = np.asarray(d, dtype=np.int16)\n",
    "plt.imshow(e, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    op = server.step('right', grayscale=True, downsample=4, min_y=45, max_y=230)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Mar 26 10:10:11 2020\n",
    "\n",
    "@author: mateu\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import imageio\n",
    "from python_server_FCEUX import Server\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "print(_get_available_gpus())\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "\n",
    "####VERIFY IF GPU IS RUNNING.\n",
    "#print(device_lib.list_local_devices())\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "import keras\n",
    "\n",
    "import tensorflow.compat.v1 as tf1\n",
    "\n",
    "'''\n",
    "config = tf1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf1.Session(config=config) \n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "'''\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    " ' Huber loss.\n",
    " ' https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\n",
    " ' https://en.wikipedia.org/wiki/Huber_loss\n",
    "'''\n",
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "  error = y_true - y_pred\n",
    "  cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "\n",
    "  squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "\n",
    "  return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "'''\n",
    " ' Same as above but returns the mean loss.\n",
    "'''\n",
    "def huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n",
    "  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))\n",
    "\n",
    "score = []\n",
    "\n",
    "TRAIN = True\n",
    "OBSERVER = 100\n",
    "UPDATE_TARGET_MODEL = 1000\n",
    "qntUpdate=0\n",
    "tamMemoryK = 150 # O tamanho da memoria será esse valor multiplicado por 1000.\n",
    "\n",
    "game = 'SuperMarioBross'\n",
    "diretorioInProcess = \"./currentProcessICMinih/\"+game+\"/\"\n",
    "total_reward_game = []\n",
    "data_average_reward = []\n",
    "trainsPerEpisode = []\n",
    "\n",
    "class Agent(): \n",
    "    def __init__(self, state_size, action_size,):\n",
    "        self.weight_backup      = diretorioInProcess+game+\".h5\"\n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=tamMemoryK*1000) if TRAIN else deque(maxlen=5)\n",
    "        self.learning_rate      = 0.0005\n",
    "        self.gamma              = 0.97\n",
    "        self.exploration_rate   = .4\n",
    "        self.exploration_min    = 0.05\n",
    "        self.exploration_decay  = 1.0e-5\n",
    "        self.k_frames           = 4\n",
    "        self.frame_height       = self.state_size[0]\n",
    "        self.frame_width        = self.state_size[1]\n",
    "        self.brain              = self._build_model()\n",
    "        self.brain_target       = self._build_model()\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Neural Net for Deep-Q learning Model\n",
    "            model = Sequential()\n",
    "                \n",
    "            #Criar camada de convolução.\n",
    "            #-> (32,84,84,4)\n",
    "            model.add(Conv2D(32, (8,8), strides=4,  input_shape = (self.state_size[0], self.state_size[1], self.k_frames) , activation = 'relu', padding='valid'))\n",
    "            #<- (84,84)\n",
    "            #->(84,84)\n",
    "            #model.add(MaxPooling2D(pool_size = (2,2), dim_ordering='th'))\n",
    "            #<-(42,42)\n",
    "            #->(42,42)\n",
    "            model.add(Conv2D(64, (4,4), strides=2, activation = 'relu', padding='valid'))\n",
    "            #<-(42,42)\n",
    "            #->(42,42)\n",
    "            #model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "            #<-(21,21)\n",
    "            #->(21,21)\n",
    "            model.add(Conv2D(64, (3,3), strides=1, activation = 'relu', padding='valid'))\n",
    "            #model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "            #<-(11,11)\n",
    "            #model.add(Conv2D(512, (7,7), strides=1, activation = 'relu', padding='same'))\n",
    "            #model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            #Criação da rede Neural.\n",
    "            model.add(Dense(512, activation='relu'))\n",
    "            #model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "            #model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "            #model.add(Dropout(.5))\n",
    "            model.add(Dense(self.action_size, activation='linear'))\n",
    "            model.compile(loss=huber_loss_mean, optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            model.load_weights(self.weight_backup)\n",
    "            self.exploration_rate = self.exploration_min\n",
    "            self.load_data()\n",
    "        return model\n",
    "    \n",
    "    def load_data(self):\n",
    "        global data_average_reward, total_reward_game, trainsPerEpisode\n",
    "        if os.path.isdir(diretorioInProcess):\n",
    "            with open(diretorioInProcess+\"avarageProcess.txt\", \"rb\") as avarageDocument:\n",
    "                data_average_reward = pickle.load(avarageDocument)\n",
    "            with open(diretorioInProcess+\"lastRewards.txt\", \"rb\") as lastRewards:\n",
    "                total_reward_game = pickle.load(lastRewards)\n",
    "            with open(diretorioInProcess+\"trainsPerEpisode.txt\", \"rb\") as trainsPerEpisodeDocument:\n",
    "                trainsPerEpisode = pickle.load(trainsPerEpisodeDocument)\n",
    "    \n",
    "            print('LastRewards: {}'.format(total_reward_game))\n",
    "            print('Average: {}'.format(data_average_reward))\n",
    "            \n",
    "            input()\n",
    "    \n",
    "    def save_model(self, avarageList, lastRewards, trainsPerEpisode):   \n",
    "       \n",
    "        if not os.path.isdir(diretorioInProcess):\n",
    "            os.makedirs(diretorioInProcess)\n",
    "            \n",
    "        self.brain.save(self.weight_backup)\n",
    "         \n",
    "        with open(diretorioInProcess+\"avarageProcess.txt\", \"wb\") as avarageDocument:\n",
    "            pickle.dump(avarageList, avarageDocument)\n",
    "            \n",
    "        with open(diretorioInProcess+\"lastRewards.txt\", \"wb\") as lastRewardsDocument:\n",
    "            pickle.dump(lastRewards, lastRewardsDocument)\n",
    "        \n",
    "        with open(diretorioInProcess+\"trainsPerEpisode.txt\", \"wb\") as trainsPerEpisodeDocument:\n",
    "            pickle.dump(trainsPerEpisode, trainsPerEpisodeDocument)\n",
    "        \n",
    "        \n",
    "    def get_last_k_frames(self, state):\n",
    "        frames = np.empty((self.k_frames, self.frame_height, self.frame_width))\n",
    "\n",
    "        for i in range(0, self.k_frames):\n",
    "            _, _, _, n_s, _ = self.memory[len(self.memory)-(self.k_frames-i)]\n",
    "            frames[i] = n_s\n",
    "            \n",
    "        #frames[3] = state  \n",
    "        return np.transpose(frames, axes=(1,2,0))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.exploration_rate or len(self.memory) < self.k_frames+1:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        k_frames_state = self.get_last_k_frames(state)\n",
    "        k_frames_state = np.expand_dims(k_frames_state, axis=0)\n",
    "        act_values = self.brain.predict(k_frames_state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    \n",
    "    def pack_K_frames(self, sample_batch_size):\n",
    "        \n",
    "        if len(self.memory) < self.k_frames+2:\n",
    "            return\n",
    "        \n",
    "        state = np.empty((sample_batch_size, self.k_frames, self.frame_height, self.frame_width))\n",
    "        action = np.empty(sample_batch_size, dtype=np.uint8)\n",
    "        reward = np.empty(sample_batch_size, dtype=np.float32)\n",
    "        next_state = np.empty((sample_batch_size, self.k_frames, self.frame_height, self.frame_width))\n",
    "        done = np.empty(sample_batch_size, dtype=np.bool)\n",
    "        \n",
    "        for k in range(sample_batch_size):\n",
    "            index = random.randint(0, len(self.memory)-self.k_frames-2)\n",
    "            for i, idx_memory in enumerate(range(index, index+self.k_frames)):\n",
    "                s, a, r, n_s, d = self.memory[idx_memory]\n",
    "            \n",
    "                state[k][i] = s\n",
    "                next_state[k][i] = n_s\n",
    "                \n",
    "            done[k] = d\n",
    "            action[k] = a\n",
    "            reward[k] = r\n",
    "    \n",
    "        #State = (32,4,84,84)  -> State Transpose = (32,84,84,4) \n",
    "        return np.transpose(state, axes=(0,2,3,1)), action, reward, np.transpose(next_state, axes=(0,2,3,1)), done\n",
    "        \n",
    "\n",
    "    def replay(self, sample_batch_size):\n",
    "       \n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        #sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        state, action, reward, next_state, done = self.pack_K_frames(sample_batch_size)\n",
    "        \n",
    "        #print('State: {}'.format(state.shape))\n",
    "        #print('Action: {}'.format(action.shape))\n",
    "        #print('Reward: {}'.format(reward.shape))\n",
    "        #print('Next_State: {}'.format(next_state.shape))\n",
    "        #print('Done: {}'.format(done.shape))\n",
    "        #input()\n",
    "        #target = reward\n",
    "        predicted = self.brain_target.predict(next_state) #Previsão proximo estado.\n",
    "        target_f = self.brain.predict(state) #Previsão estado atual.\n",
    "        #print('Predicted: {}'.format(predicted))\n",
    "        #print('Predicted[0]: {}'.format(predicted[2]))\n",
    "        #print('Predicted Max: {}'.format(np.amax(predicted)))\n",
    "        #print('Reward: {}'.format(reward))\n",
    "        #print('Target_f: {}'.format(target_f))\n",
    "        #input()\n",
    "        \n",
    "        for i in range(sample_batch_size):\n",
    "            target = reward[i] + (self.gamma * np.amax(predicted[i]) * (1-done[i]))\n",
    "            target_f[i][action[i]] = target\n",
    "            #print('Action: {}'.format(action[i]))\n",
    "            #print('target: {}'.format(target))\n",
    "            #print('target_f: {}'.format(target_f[i]))\n",
    "            #input()\n",
    "            \n",
    "        \n",
    "        #print('Target_f Formatado: {}'.format(target_f))\n",
    "        #input()\n",
    "        history = self.brain.fit(state, target_f, batch_size=sample_batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate -= self.exploration_decay\n",
    "    \n",
    "        return history, self.exploration_rate\n",
    "\n",
    "    def update_target_model(self, frame_number):\n",
    "        if (frame_number % UPDATE_TARGET_MODEL == 0 and TRAIN and frame_number > OBSERVER):\n",
    "            self.brain_target.set_weights(self.brain.get_weights())\n",
    "            global qntUpdate\n",
    "            qntUpdate+=1\n",
    "            print('-------------------------UPDATED TARGET MODEL({}) ------------------------------'.format(qntUpdate))\n",
    "\n",
    "\n",
    "class Nintendo():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = Server()\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes = 5000000\n",
    "        self.action_size = 3\n",
    "        self.state_size = (80,80)\n",
    "        self.agent = Agent(self.state_size, self.action_size)\n",
    "        self.number_print = 0\n",
    "        self.best_score = -99999999\n",
    "        self.replay_bestPlay = deque(maxlen=300)\n",
    "        self.crop_on_top = 35 #57\n",
    "        self.crop_on_bottom = 15\n",
    "        self.crop_on_border = 7\n",
    "        self.frame_number=0\n",
    "        self.freq_update=4\n",
    "        self.qnt_train=0\n",
    "        self.epochs_to_save = 50\n",
    "        self.time_train_init = time.time()\n",
    "\n",
    "        self.frames_in_atual_episode=0\n",
    "        self.seconds_in_atual_episode=0\n",
    "    \n",
    "    def actions(self,n):\n",
    "        if n > 5:\n",
    "            print('Numero maior que quantidade de buttões')\n",
    "            return\n",
    "        return ['left', 'right', 'A'][n]\n",
    "    \n",
    "    def restart_chronometer(self):\n",
    "        self.frames_in_atual_episode=0\n",
    "        self.seconds_in_atual_episode=0\n",
    "    \n",
    "    def get_frames_per_seconds_in_atual_episode(self):\n",
    "        return int(self.frames_in_atual_episode / (time.time() - self.seconds_in_atual_episode))\n",
    "    \n",
    "    def to_gray_scale(self, img):\n",
    "        return 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\n",
    "    \n",
    "    def down_sample(self, img):\n",
    "        return img[::2,::2]\n",
    "    \n",
    "    def preprocess_img(self, img):        \n",
    "        new_image = cv2.resize(np.asarray(img, dtype=np.float32)/ 255, self.state_size, interpolation=cv2.INTER_AREA)\n",
    "        '''plt.imshow(new_image)\n",
    "        plt.show()\n",
    "        print('Shape: {}'.format(new_image.shape))\n",
    "        input()'''\n",
    "        return new_image\n",
    "        \n",
    "    def crop_img(self, img):\n",
    "        #return img[self.crop_on_top: -self.crop_on_bottom, self.crop_on_border:-self.crop_on_border]\n",
    "        return img[35:195, 8:-8]\n",
    "  \n",
    "    def transform_reward(self, reward):\n",
    "        return np.sign(reward)\n",
    "      \n",
    "    def formatTimeBr(time):\n",
    "        horas = time/3600\n",
    "        minutos = (time%3600)/60\n",
    "        segundos = (time%3600)%60\n",
    "        \n",
    "        return str('{}hr : {}min : {}seg'.format(horas, minutos, segundos))\n",
    "    \n",
    "    def save_image_epoch(self, gif_to_save, epoch, r):\n",
    "        \n",
    "        if (r == 1):\n",
    "            diretorio = diretorioInProcess+'/movies/'\n",
    "            if not os.path.isdir(diretorio):\n",
    "                os.makedirs(diretorio)\n",
    "                \n",
    "            frames_per_seconds_gif = 60\n",
    "            seconds = len(gif_to_save)/frames_per_seconds_gif\n",
    "    \n",
    "            imageio.mimwrite(diretorio+str(epoch)+'.mp4', gif_to_save , fps = 75)\n",
    "            print('Gif Salvo') \n",
    "            \n",
    "    def run(self):\n",
    "        global total_reward_game, data_average_reward, trainsPerEpisode\n",
    "    \n",
    "        try:\n",
    "            for i_episodes in range(self.episodes):\n",
    "                js = self.env.reset('unnecessary',grayscale=True, downsample=2, min_y=45, max_y=230)\n",
    "                state = js['matriz']\n",
    "               \n",
    "                state = self.preprocess_img(state)\n",
    "                \n",
    "                done = False\n",
    "                atual_epoch = []\n",
    "                total_reward=0\n",
    "                history_list = []\n",
    "                exploration = 1.0\n",
    "                self.seconds_in_atual_episode= time.time()\n",
    "                last_reward = 0\n",
    "                \n",
    "                while not done:\n",
    "                                        \n",
    "                    #if i_episodes > 600 or not TRAIN:\n",
    "                    #self.env.render()\n",
    "                    action = self.agent.act(state)\n",
    "                    js = self.env.step(self.actions(action), grayscale=True, downsample=2, min_y=45, max_y=230)\n",
    "                    next_state = js['matriz']\n",
    "                    if js['reward'] > last_reward:\n",
    "                        reward = 1\n",
    "                    elif js['reward'] == last_reward:\n",
    "                        reward = 0\n",
    "                    else:\n",
    "                        reward = -1\n",
    "                    last_reward = js['reward']\n",
    "                    done = True if js['endgame'] != 8 else False\n",
    "                    \n",
    "                    total_reward+=reward\n",
    "                    #self.replay_bestPlay.append(next_state)\n",
    "                    #if total_reward >= 5:\n",
    "                    #    self.smile_for_the_photo()\n",
    "                    #if (i_episodes % self.epochs_to_save == 0):\n",
    "                    #    atual_epoch.append(np.asarray(next_state))\n",
    "                    next_state = self.preprocess_img(next_state)\n",
    "                    self.agent.remember(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "                    \n",
    "                    self.frame_number+=1\n",
    "                    self.frames_in_atual_episode+=1\n",
    "                    if TRAIN:\n",
    "                        if self.frame_number > OBSERVER and self.frame_number % 4 == 0:\n",
    "                            history, exploration = self.agent.replay(self.sample_batch_size)\n",
    "                            history_list.append(history.history['loss'])    \n",
    "                            self.qnt_train+=1\n",
    "                        \n",
    "                        self.agent.update_target_model(self.frame_number)\n",
    "    \n",
    "                total_reward_game.append(total_reward)\n",
    "                mediaUltimosJogos = np.mean(total_reward_game[-50:])\n",
    "                trainsPerEpisode.append(self.qnt_train)\n",
    "                data_average_reward.append(mediaUltimosJogos)\n",
    "                if TRAIN and self.frame_number > OBSERVER:\n",
    "                    #self.save_image_epoch(atual_epoch, i_episodes, reward)   \n",
    "                    if mediaUltimosJogos > self.best_score and self.frame_number > OBSERVER:\n",
    "                            self.best_score = mediaUltimosJogos\n",
    "                            self.agent.save_model(data_average_reward, total_reward_game, trainsPerEpisode)\n",
    "                            print('Save -> ', end='')\n",
    "                print(\"Episode {}# r: {}# Average: {:.3}# Loss: {:.6} # Train: {} # eps: {:.3}# Space: {}% 'fps: {}:\".format(i_episodes, total_reward, data_average_reward[len(data_average_reward)-1], np.mean(history_list), self.qnt_train, exploration, round(((len(self.agent.memory) / (tamMemoryK*1000)) * 100), 2), self.get_frames_per_seconds_in_atual_episode() ))\n",
    "                self.restart_chronometer()\n",
    "        finally:\n",
    "            plt.plot(trainsPerEpisode, data_average_reward)\n",
    "            plt.xlabel('Trains')\n",
    "            plt.ylabel('Reward')\n",
    "            \n",
    "            #plt.title('Time of train: (Minih) {}'.format(time.time() - self.time_train_init))\n",
    "            #print(self.formatTimeBr((time.time() - s\\elf.time_train_init)))\n",
    "            if TRAIN:\n",
    "                self.agent.save_model(data_average_reward, total_reward_game, trainsPerEpisode)\n",
    "            plt.savefig('atualGraph.png')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    nintendo = Nintendo()\n",
    "    nintendo.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(((1,2,3), (3,2,1)), dtype=np.float32)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [1,2,3,4,5]\n",
    "lista.reverse()\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
